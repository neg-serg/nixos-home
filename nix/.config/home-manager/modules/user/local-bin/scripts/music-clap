#!/usr/bin/env python3
"""Compute CLAP audio embeddings and optional text similarities.

Usage:
  music-clap [options] [PATH ...]

Description:
  Loads a LAION-CLAP model, extracts embeddings for provided audio files or
  directories, and optionally compares them to text prompts. Results can be
  emitted in human-readable form, JSON, or dumped as .npy vectors.
"""
import argparse
import json
import math
import os
import sys
from pathlib import Path
from typing import Iterable, List, Sequence

import numpy as np

torch_available = False
try:  # torch might fail to import if CUDA libs missing; defer error until use.
    import torch
    torch_available = True
except Exception as exc:  # pragma: no cover
    torch = None  # type: ignore

try:  # optional faster JSON
    import orjson  # type: ignore
except Exception:  # pragma: no cover
    orjson = None

try:
    from laion_clap import CLAP_Module
except Exception as exc:  # pragma: no cover
    print("[music-clap] laion_clap module not available. Ensure python env includes laion-clap.", file=sys.stderr)
    raise

AUDIO_EXTS = {
    ".mp3",
    ".flac",
    ".wav",
    ".ogg",
    ".m4a",
    ".opus",
    ".aac",
    ".wma",
    ".aiff",
    ".aif",
}
DEFAULT_TEXTS: Sequence[str] = ()


def walk_inputs(paths: Iterable[Path]) -> List[Path]:
    files: List[Path] = []
    for p in paths:
        if p.is_dir():
            for child in sorted(p.rglob("*")):
                if child.is_file() and child.suffix.lower() in AUDIO_EXTS:
                    files.append(child)
        elif p.is_file():
            files.append(p)
    return files


def normalize(vec: np.ndarray) -> np.ndarray:
    norm = float(np.linalg.norm(vec))
    if norm == 0.0:
        return vec
    return vec / norm


def cosine(a: np.ndarray, b: np.ndarray) -> float:
    return float(np.dot(normalize(a), normalize(b)))


def load_model(args: argparse.Namespace) -> CLAP_Module:
    if not torch_available:
        raise RuntimeError("PyTorch not available; install torch/torchaudio for laion-clap")
    device = args.device
    if device == "auto":
        device = "cuda:0" if torch.cuda.is_available() else "cpu"
    enable_fusion = args.fusion
    model = CLAP_Module(enable_fusion=enable_fusion, device=device, amodel=args.amodel)
    ckpt = args.ckpt
    model_id = args.model_id
    if ckpt:
        ckpt = os.path.expanduser(ckpt)
        if not Path(ckpt).exists():
            print(f"[music-clap] checkpoint not found: {ckpt}", file=sys.stderr)
            sys.exit(2)
        model.load_ckpt(ckpt=ckpt, model_id=-1, verbose=not args.quiet)
    else:
        model.load_ckpt(model_id=model_id, verbose=not args.quiet)
    return model


def dump_embedding(path: Path, embedding: np.ndarray, dump_dir: Path) -> Path:
    dump_dir.mkdir(parents=True, exist_ok=True)
    target = dump_dir / (path.stem + ".npy")
    np.save(target, embedding)
    return target


def emit_json(data: dict) -> None:
    payload = orjson.dumps(data).decode("utf-8") if orjson else json.dumps(data, ensure_ascii=False)
    print(payload)


def emit_human(entry: dict, top_text: int) -> None:
    path = entry.get("path", "<unknown>")
    print(path)
    if "embedding_path" in entry:
        print(f"  stored: {entry['embedding_path']}")
    matches = entry.get("text_matches", [])
    if matches:
        limit = matches if top_text < 1 else matches[:top_text]
        for item in limit:
            print(f"  {item['score']:.3f}\t{item['text']}")


def parse_args() -> argparse.Namespace:
    ap = argparse.ArgumentParser(description="Extract CLAP embeddings for audio files")
    ap.add_argument("paths", nargs="*", help="audio files or directories")
    ap.add_argument("--text", action="append", dest="texts", help="text prompt to compare against (repeatable)")
    ap.add_argument("--top", type=int, default=5, help="top-N text matches to display per track")
    ap.add_argument("--dump", type=Path, help="directory to store embeddings as .npy")
    ap.add_argument("--json", action="store_true", help="emit JSON per track")
    ap.add_argument("--device", default="auto", help="torch device (default: auto)")
    ap.add_argument("--amodel", default="HTSAT-tiny", help="audio encoder architecture (default: HTSAT-tiny)")
    ap.add_argument("--fusion", action="store_true", help="enable fusion model variant")
    ap.add_argument("--model-id", type=int, default=-1, help="pretrained checkpoint id to download (default: 3 fusion / 1 non-fusion)")
    ap.add_argument("--ckpt", type=str, help="path to a custom checkpoint (skips download)")
    ap.add_argument("--include-embedding", action="store_true", help="include embedding vector in output (JSON only)")
    ap.add_argument("--quiet", action="store_true", help="suppress model load logging")
    return ap.parse_args()


def main() -> int:
    args = parse_args()
    inputs = [Path(p).expanduser() for p in args.paths]
    if not inputs:
        print("[music-clap] no inputs provided", file=sys.stderr)
        return 1
    files = walk_inputs(inputs)
    if not files:
        print("[music-clap] no audio files found", file=sys.stderr)
        return 1

    model = load_model(args)

    results = model.get_audio_embedding_from_filelist([str(p) for p in files], use_tensor=False)
    if not isinstance(results, np.ndarray):
        results = np.asarray(results)
    text_prompts = args.texts or list(DEFAULT_TEXTS)
    text_embeds = None
    if text_prompts:
        text_embeds = model.get_text_embedding(text_prompts, use_tensor=False)
        text_embeds = np.asarray(text_embeds)

    for idx, audio_path in enumerate(files):
        audio_embed = results[idx]
        entry: dict = {"path": str(audio_path)}
        if args.dump:
            target = dump_embedding(audio_path, audio_embed, args.dump)
            entry["embedding_path"] = str(target)
        if args.include_embedding and args.json:
            entry["embedding"] = audio_embed.tolist()
        if text_embeds is not None:
            matches = []
            for j, prompt in enumerate(text_prompts):
                score = cosine(audio_embed, text_embeds[j])
                matches.append({"text": prompt, "score": score})
            matches.sort(key=lambda x: x["score"], reverse=True)
            entry["text_matches"] = matches
        if args.json:
            emit_json(entry)
        else:
            emit_human(entry, args.top)
    return 0


if __name__ == "__main__":  # pragma: no cover
    raise SystemExit(main())
